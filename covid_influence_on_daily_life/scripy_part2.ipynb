{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "helpful-steel",
   "metadata": {},
   "source": [
    "# Additional analysis to analysis the COVID influence on people's daily life\n",
    "\n",
    "1. Produce only one word cloud use all words collected from January - December, 2020 and 2019.\n",
    "2. Compare the wordclouds of the same keyword & subreddit between 2019 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_prefix = 'covid_influence/'\n",
    "keyword = 'covid|corona|quarantine|pandemic'\n",
    "subreddits = ['Gifts,GiftIdeas', 'personalfinance', 'jobs,careerguidence,GetEmployed', 'CasualConversation', 'depression', 'books', 'teenagers', 'parenting', 'fitness', \n",
    "                   'gaming', 'relationships']\n",
    "subm_field = ['selftext', 'title']\n",
    "cmt_field = 'body'\n",
    "\n",
    "# Manually picked stopwords.\n",
    "temp_stopword = 'covid|corona|quarantine|pandemic'.split('|')+['day', 'days', 'week', 'weeks', 'month', 'months', 'year', 'years', 'hours',  \n",
    "                                                               'could', 'would', 'should', \n",
    "                                                               'https', 'www', 'com', 'redditor', 'reddit', 'people', 'way', 'thoght', 'thoughts',\n",
    "                                                               'get', 'make', 'give', 'take', 'think', 'use', 'want', 'like', 'know', \n",
    "                                                               'say', 'look', 'come', 'ask', 'feel', 'see', 'thank', 'thanks', 'seem', 'sure', 'able', 'unable',\n",
    "                                                               'many', 'much', 'some', 'someone', 'something', 'sth', 'thing', 'things', 'anyone', 'guy', \n",
    "                                                               'every', 'might', 'may', 'maybe', 'probably', 'also', 'already', 'always', 'really',                                                               \n",
    "                                                               'first', 'last', 'lot', 'with', 'without', 'well', 'best', 'good', 'better', \n",
    "                                                               'include', 'etc', 'else', 'since', 'back', 'however', 'even', 'ago', 'pretty', \n",
    "                                                               'actual', 'actually', 'another', 'due', 'likely', 'kind', 'anymore', \n",
    "                                                               'still', 'often', 'question',  'help', 'ever', 'post', 'please', 'around', \n",
    "                                                               'coronavirus', 'lockdown', 'lockdowns', 'outbreak', 'epidemic', 'virus', 'outbreak', \n",
    "                                                               'time', 'webp', 'start']\n",
    "\n",
    "stopwords = dict()\n",
    "for subreddit in subreddits:\n",
    "    if subreddit == 'Gifts,GiftIdeas':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['gift']\n",
    "    elif subreddit == 'personalfinance':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['finance']\n",
    "    elif subreddit == 'creditcards':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['credit', 'card']\n",
    "    elif subreddit == 'realestate':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['house', 'home']\n",
    "    elif subreddit == 'smallbusiness':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['small', 'business']\n",
    "    elif subreddit == 'jobs,careerguidence,GetEmployed':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['job', 'jobs', 'work', '']\n",
    "    elif subreddit == 'books':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['read', 'book']\n",
    "    elif subreddit == 'gaming':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['game']\n",
    "    elif subreddit == 'relationships':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['relationship']\n",
    "    elif subreddit == 'china':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['chinese']\n",
    "    elif subreddit == 'india':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['indian']\n",
    "    elif subreddit == 'australia':\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')+['australian', 'australians']\n",
    "    else:\n",
    "        stopwords[subreddit] = temp_stopword+subreddit.lower().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_word_freq(pickle_path):\n",
    "    freq_all = pickle.load(open(pickle_path, 'rb'))\n",
    "    comb_freq = dict()\n",
    "    for freq in freq_all:\n",
    "        for key in freq.keys():\n",
    "            if key in comb_freq.keys():\n",
    "                comb_freq[key] += freq[key]\n",
    "            else:\n",
    "                comb_freq[key] = freq[key]\n",
    "    return comb_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_20 = dict()\n",
    "word_freq_19 = dict()\n",
    "word_freq_diff = dict()\n",
    "for subreddit in subreddits:\n",
    "    word_freq_20[subreddit] = combine_word_freq(save_path_prefix+'files/'+keyword+'_'+subreddit+'/'+'word_frequency.p')\n",
    "    word_freq_19[subreddit] = combine_word_freq(save_path_prefix+'files/_'+subreddit+'/'+'word_frequency.p')\n",
    "    # Compare the word frequency in 2020 to that in 2019\n",
    "    word_freq_diff[subreddit] = dict()\n",
    "    for key in word_freq_20[subreddit].keys():\n",
    "        if key not in word_freq_19[subreddit].keys():\n",
    "            word_freq_diff[subreddit][key] = word_freq_20[subreddit][key]\n",
    "        elif (key in word_freq_19[subreddit].keys()) and (word_freq_20[subreddit][key] - word_freq_19[subreddit][key] > 0):\n",
    "            word_freq_diff[subreddit][key] = word_freq_20[subreddit][key] - word_freq_19[subreddit][key]\n",
    "\n",
    "    for stopword in stopwords[subreddit]:\n",
    "        word_freq_19[subreddit].pop(stopword, None)\n",
    "        word_freq_20[subreddit].pop(stopword, None)\n",
    "        word_freq_diff[subreddit].pop(stopword, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_dividor(n):\n",
    "    tmp = []\n",
    "    i = 1\n",
    "    while i <= n: \n",
    "        if (n % i==0) : \n",
    "            tmp.append(i) \n",
    "        i = i + 1\n",
    "    if len(tmp) % 2 == 0:\n",
    "        return [tmp[int(len(tmp)/2-1)], tmp[int(len(tmp)/2)]]\n",
    "    else:\n",
    "        return [tmp[round(len(tmp)/2)], tmp[round(len(tmp)/2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_idx = mid_dividor(len(subreddits))\n",
    "for idx in range(2, 3):\n",
    "    if idx == 0:\n",
    "        word_freq = word_freq_19\n",
    "        fsave = save_path_prefix+'plots/wordcloud_queries_19.jpg'\n",
    "    elif idx == 1:\n",
    "        word_freq = word_freq_20\n",
    "        fsave = save_path_prefix+'plots/wordcloud_queries_20.jpg'\n",
    "    else:\n",
    "        word_freq = word_freq_diff\n",
    "        fsave = save_path_prefix+'plots/wordcloud_queries_diff.jpg'\n",
    "    \n",
    "    fig, axs = plt.subplots(sub_idx[0], sub_idx[1])\n",
    "    cnt = 0\n",
    "    for subreddit in subreddits:\n",
    "        wc = WordCloud(width = 3000, height = 2000, background_color='black', colormap='Set2').generate_from_frequencies(frequencies=word_freq[subreddit])       \n",
    "        sub = np.unravel_index(cnt, (sub_idx[0], sub_idx[1]))\n",
    "        ax = axs[sub[0], sub[1]]\n",
    "        ax.imshow(wc)\n",
    "        ax.set_title(subreddit, fontsize=20)\n",
    "        ax.spines['right'].set_visible(0)\n",
    "        ax.spines['top'].set_visible(0)\n",
    "        ax.axis('off')\n",
    "        cnt += 1\n",
    "        del wc, ax, sub\n",
    "    \n",
    "    fig.set_size_inches(17*sub_idx[0], 5*sub_idx[1])\n",
    "    plt.savefig(fsave, bbox_inches='tight')\n",
    "    del word_freq, fsave, fig, axs, cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
