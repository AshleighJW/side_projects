{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How COVID Influenced People's lives\n",
    "\n",
    "COVID has a great negative influence on the society. Nearly nine-in-ten U.S. adults say their life has changed at least a little as a result of the COVID-19 outbreak, including 44% who say their life has changed in a major way ([Most Americans Say Coronavirus Outbreak Has Impacted Their Lives](https://www.pewsocialtrends.org/2020/03/30/most-americans-say-coronavirus-outbreak-has-impacted-their-lives/)). This study quantifies how COVID has influenced many aspects of people's everyday life by visualizing keywords changes in subreddits concerning daily life topics during the past year (in case you are not familiar with reddit: [subreddit is a web forum of a particular topic where you can post links or create a self post and dicuss.](https://www.reddit.com/r/help/comments/37shum/what_is_a_subreddit/)).\n",
    "\n",
    "### Structure\n",
    "What's included in the analysis: trace the number of members of each subreddit and the keywords change.\n",
    "\n",
    "### Data source\n",
    "The data is scrapped from one of the biggest online community **reddit**. **Reddit** has more than [430m users](https://au.oberlo.com/blog/reddit-statistics), [168b views per year](https://mediakix.com/blog/reddit-statistics-users-demographics/)).\n",
    "\n",
    "[Aother good statistics](https://websitebuilder.org/blog/reddit-statistics/).\n",
    "\n",
    "### Subreddits selected\n",
    "**Selection criteria**\n",
    "- Focus on a daily life related topics, as listed separatly below.\n",
    "- Among all subreddits with similar topic, I select the one or two with most members to idealize the size and variety of the data.\n",
    "\n",
    "**List of subreddits (click the name to view the subreddit)**\n",
    "\n",
    "- **Food**   \n",
    "[r/cooking](https://www.reddit.com/r/Cooking/): 2.5m, [r/recipes](https://www.reddit.com/r/recipes/): 2.4m, [r/MealPrepSunday](https://www.reddit.com/r/MealPrepSunday/): 2m, [r/budgetfood](https://www.reddit.com/r/budgetfood/): 302k\n",
    "\n",
    "- **Family life, childcare and relationships**  \n",
    "[r/Parenting](https://www.reddit.com/r/Parenting/): 3.2m, [r/relationships](https://www.reddit.com/r/relationships/): 3m, [r/teenagers](https://www.reddit.com/r/teenagers/): 2.4m, [r/childfree](https://www.reddit.com/r/childfree/): 1.4m, [r/weddingplanning](https://www.reddit.com/r/weddingplanning/): 144k, [r/family](https://www.reddit.com/r/family/): 138k, [r/education](https://www.reddit.com/r/education/new/): 124k, [r/christmas](https://www.reddit.com/r/christmas/): 100k, [r/Gifts](https://www.reddit.com/r/Gifts/): 56.2k, [r/GiftIdeas](https://www.reddit.com/r/GiftIdeas/): 45.4k\n",
    "\n",
    "- **Hobbies**  \n",
    "[r/gaming](https://www.reddit.com/r/gaming/): 29.3m, [r/books](https://www.reddit.com/r/books/): 19m, [r/camping](https://www.reddit.com/r/camping/): 1.9m, [r/Fitness](https://www.reddit.com/r/Fitness/): 8m, [r/travel](https://www.reddit.com/r/travel/): 5.7m\n",
    "\n",
    "- **Financial status**\n",
    "[r/personalfinance](https://www.reddit.com/r/personalfinance/): 14.3m, [r/Economics](https://www.reddit.com/r/Economics/): 1.2m, [r/povertyfinance](https://www.reddit.com/r/povertyfinance/): 588k, [r/CreditCards](https://www.reddit.com/r/CreditCards/): 262k, [r/realestateinvesting](https://www.reddit.com/r/realestateinvesting/): 330k, [r/RealEstate](https://www.reddit.com/r/RealEstate/): 236k\n",
    "\n",
    "- **Career**\n",
    "[r/Entrepreneur](https://www.reddit.com/r/Entrepreneur/): 889k, [r/business](https://www.reddit.com/r/business/): 602k, [r/smallbusiness](https://www.reddit.com/r/smallbusiness/): 484k, [r/jobs](https://www.reddit.com/r/jobs/): 428k, [r/WorkOnline](https://www.reddit.com/r/WorkOnline/): 333k, [r/careerguidance](https://www.reddit.com/r/careerguidance/): 250k, [r/GetEmployed](https://www.reddit.com/r/GetEmployed/): 102k\n",
    "\n",
    "- **Mental Health**\n",
    "[r/GetMotivated](https://www.reddit.com/r/GetMotivated/): 17.0m, [r/CasualConversation](https://www.reddit.com/r/CasualConversation/): 1.4m, [r/psychology](https://www.reddit.com/r/psychology/): 809k, [r/getdisciplined](https://www.reddit.com/r/getdisciplined/): 729k, [r/depression](https://www.reddit.com/r/depression/): 725k, [r/Anxiety](https://www.reddit.com/r/Anxiety/): 429k, [r/mentalhealth](https://www.reddit.com/r/mentalhealth/): 220k, [r/Lonely](https://www.reddit.com/r/lonely/): 194k\n",
    "\n",
    "- **Others**\n",
    "[r/Futurology](https://www.reddit.com/r/Futurology/): 15.1m, [r/Coronavirus](https://www.reddit.com/r/Coronavirus/): 2.4m, [r/legaladvice](https://www.reddit.com/r/legaladvice/): 1.4m\n",
    "\n",
    "### Major tool\n",
    "- **Pushshift** ([Manual](https://reddit-api.readthedocs.io/en/latest/), [Github](https://github.com/pushshift/api), [r/Pushshift](https://www.reddit.com/r/pushshift/))  \n",
    "\n",
    "Pushshift is used to scrapping data from reddit. Compared to the popular tool [PRAW](https://praw.readthedocs.io/en/latest/), Pushshift provides a more powerful way to search the submissions and comments, especially for the date related search queries. In addtion, Pushshift returns query results quicker when the search invloves large amount of data.\n",
    "\n",
    "On the other hand, the research results from Pushshift have a couple day delay - meaning that the lastest reddit data you can get using Pushshift is about a couple days ago. This analysis, though, is not real-time, therefore I used Pushshift to get the data I need for this analysis.\n",
    "\n",
    "- **[WordCloud](https://amueller.github.io/word_cloud/index.html)**  \n",
    "\n",
    "WordCloud is used to visualize the keywords of a specific (group of) subreddit(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_prefix = 'covid_influence/files/'\n",
    "query_keywords = ['']\n",
    "# query_keywords = ['covid|corona|quarantine|pandemic'] # Use '+' or '|' to connect multiple keywords; leave as [''] without searching for specific keywords\n",
    "query_subreddits = ['Gifts,GiftIdeas', 'travel', 'personalfinance', 'creditcards', 'realestate', 'smallbusiness', 'jobs,careerguidence,GetEmployed', \n",
    "                   'GetMotivated', 'CasualConversation', 'depression', 'anxiety', 'mentalhealth', 'Lonely', 'books', 'teenagers', 'parenting', 'fitness', \n",
    "                   'AskAnAmerican', 'gaming', 'relationships', 'china', 'india', 'unitedkingdom', 'australia'] # Use ',' to connect multiple subreddits, leave as [''] if search across all subreddits\n",
    "query_date_ranges = []\n",
    "for m in range(1, 13):\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-01', '2020-'+str(m).zfill(2)+'-05'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-06', '2020-'+str(m).zfill(2)+'-10'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-11', '2020-'+str(m).zfill(2)+'-15'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-16', '2020-'+str(m).zfill(2)+'-20'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-21', '2020-'+str(m).zfill(2)+'-25'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-26', '2020-'+str(m).zfill(2)+'-28'])\n",
    "\n",
    "# Parameters to plot the wordclouds\n",
    "name_months = ['Jan.', 'Feb.', 'Mar.', 'Apr.', 'May.', 'June', 'July', 'Aug.', 'Sept.', 'Oct.', 'Nov.', 'Dec.']\n",
    "plt_cfg = dict()\n",
    "plt_cfg['path_save'] = 'covid_influence/plots/'\n",
    "plt_cfg['size'] = (50, 25)\n",
    "plt_cfg['xSub'] = 3\n",
    "plt_cfg['ySub'] = 4\n",
    "plt_cfg['title'] = [month for month in name_months]\n",
    "\n",
    "# Parameters to fetch submissions and comments\n",
    "cfg_subm = dict()\n",
    "cfg_subm['field'] = 'title,selftext,num_comments,author'\n",
    "cfg_subm['rm_dupe'] = 'title'\n",
    "cfg_subm['sort'] = 'num_comments'\n",
    "cfg_subm['sort_type'] = 'desc'\n",
    "cfg_subm['query_type'] = 'submission'\n",
    "\n",
    "cfg_cmt = dict()\n",
    "cfg_cmt['field'] = 'body,author,score'\n",
    "cfg_cmt['rm_dupe'] = 'body'\n",
    "cfg_cmt['sort'] = 'score'\n",
    "cfg_cmt['sort_type'] = 'desc'\n",
    "cfg_cmt['query_type'] = 'comment'\n",
    "\n",
    "# Add additional stopwords\n",
    "add_stopwords = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'day', 'week', 'month', 'year', 'thing', 'app', 'new', 'old', \n",
    "                 'hundred', 'thousand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcs_pushshift\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting submissions and comments from reddit using [Pushshift](https://reddit-api.readthedocs.io/en/latest/#comments-search)\n",
    "\n",
    "The original code of the function *GetPushshiftData* is from [\n",
    "dylankilkenny/PushShift.py](https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b) and [Rare Loot](https://rareloot.medium.com/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563).\n",
    "\n",
    "### 1. Assemble queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = []\n",
    "fname = []\n",
    "query_main = []\n",
    "query_subm = []\n",
    "query_cmt = []\n",
    "suffix_subm = '&filter='+cfg_subm['field']+'&sort_type='+cfg_subm['sort']+'&sort=desc'+'&size=100'\n",
    "suffix_cmt = '&filter='+cfg_cmt['field']+'&sort_type='+cfg_cmt['sort']+'&sort=desc'+'&size=100'\n",
    "for keyword in query_keywords:\n",
    "    query_temp = 'q='+keyword\n",
    "    for subreddit in query_subreddits:\n",
    "        query_main.append(keyword+'_'+subreddit)\n",
    "        for date_range in query_date_ranges:\n",
    "            query_subm.append(query_temp+'&subreddit='+subreddit+'&after='+ date_range[0]+'&before='+date_range[1]+suffix_subm)\n",
    "            query_cmt.append(query_temp+'&subreddit='+subreddit+'&after='+ date_range[0]+'&before='+date_range[1]+suffix_cmt)\n",
    "            save_path.append(save_path_prefix+keyword+'_'+subreddit+'/')\n",
    "            fname.append(date_range[0]+'_'+date_range[1])\n",
    "    del query_temp\n",
    "del suffix_subm, suffix_cmt, keyword, subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code\n",
    "# subms_all = []\n",
    "# cmts_all = []\n",
    "# for idx in range(len(fname)):\n",
    "#     try:\n",
    "#         subms_all.append(pd.read_csv(save_path[idx]+cfg_subm['query_type']+'_'+fname[idx]+'.csv'))\n",
    "#     except:\n",
    "#         subms_all.append([])\n",
    "#     try:\n",
    "#         cmts_all.append(pd.read_csv(save_path[idx]+cfg_cmt['query_type']+'_'+fname[idx]+'.csv'))\n",
    "#     except:\n",
    "#         cmts_all.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collect the submissions and comments from Pushshift server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subms_all = []\n",
    "cmts_all = []\n",
    "for idx in range(0, len(fname)):\n",
    "    print('> Processing query: '+str(idx+1)+' / '+str(len(query_subm)) + '. Save to: '+save_path[idx]+fname[idx]+'/')\n",
    "    cfg_subm['path_save'] = save_path[idx]\n",
    "    cfg_subm['save_suffix'] = fname[idx]\n",
    "    df_subm = funcs_pushshift.fetch_data(query_subm[idx], cfg_subm)\n",
    "    subms_all.append(df_subm)\n",
    "\n",
    "    cfg_cmt['path_save'] = save_path[idx]\n",
    "    cfg_cmt['save_suffix'] = fname[idx]\n",
    "    df_cmt = funcs_pushshift.fetch_data(query_cmt[idx], cfg_cmt)\n",
    "    cmts_all.append(df_cmt)\n",
    "    del df_subm, df_cmt\n",
    "del query_subm, query_cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assemble text of each month and calculate the number of active redditors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_dividor(n):\n",
    "    tmp = []\n",
    "    i = 1\n",
    "    while i <= n: \n",
    "        if (n % i==0) : \n",
    "            tmp.append(i) \n",
    "        i = i + 1\n",
    "    if len(tmp) % 2 == 0:\n",
    "        return [tmp[int(len(tmp)/2-1)], tmp[int(len(tmp)/2)]]\n",
    "    else:\n",
    "        return [tmp[round(len(tmp)/2)], tmp[round(len(tmp)/2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for plotting\n",
    "if not os.path.exists(plt_cfg['path_save']):\n",
    "    os.makedirs(plt_cfg['path_save'])\n",
    "sub_idx = mid_dividor(len(query_main))\n",
    "fig, axs = plt.subplots(sub_idx[0], sub_idx[1])\n",
    "\n",
    "# Get column names\n",
    "subm_cols = cfg_subm['field'].split(',')\n",
    "cmt_cols = cfg_cmt['field'].split(',')\n",
    "# Combine the data from the same month for each year and calculate the number of active redditors\n",
    "num_files_month = int(len(fname) / (len(name_months) * len(query_keywords) * len(query_subreddits)))\n",
    "subms = dict()\n",
    "cmts = dict()\n",
    "idx_data = 0\n",
    "for cnt_query in range(len(query_main)):\n",
    "    subms[query_main[cnt_query]] = []\n",
    "    cmts[query_main[cnt_query]] = []\n",
    "    num_redditor = []\n",
    "    num_subm = []\n",
    "    num_cmt = []\n",
    "    for idx in range(0, len(name_months)):\n",
    "        subm_temp = []\n",
    "        cmt_temp = []\n",
    "        for ii in range(0, num_files_month):\n",
    "            if len(subms_all[idx_data]) != 0:\n",
    "                subm_temp.append(subms_all[idx_data])\n",
    "            if len(cmts_all[idx_data]) != 0:\n",
    "                cmt_temp.append(cmts_all[idx_data])\n",
    "            idx_data += 1\n",
    "        if subm_temp == []:\n",
    "            subms[query_main[cnt_query]].append(pd.DataFrame(columns=subm_cols))\n",
    "        else:\n",
    "            subms[query_main[cnt_query]].append(pd.concat(subm_temp).reset_index()[subm_cols])\n",
    "        if cmt_temp == []:\n",
    "            cmts[query_main[cnt_query]].append(pd.DataFrame(columns=subm_cols))\n",
    "        else:\n",
    "            cmts[query_main[cnt_query]].append(pd.concat(cmt_temp).reset_index()[cmt_cols])\n",
    "\n",
    "        # Calculate the number of redditors\n",
    "        authors = pd.concat([subms[query_main[cnt_query]][idx]['author'], cmts[query_main[cnt_query]][idx]['author']])\n",
    "        temp = authors.shape[0]\n",
    "        authors = authors.drop_duplicates().reset_index()\n",
    "        num_redditor.append(authors.shape[0])\n",
    "        if len(subms[query_main[cnt_query]][idx]) == 0:\n",
    "            num_subm.append(0)\n",
    "        else:\n",
    "            num_subm.append(len(subms[query_main[cnt_query]][idx]))\n",
    "        num_cmt.append(len(cmts[query_main[cnt_query]][idx]))\n",
    "        del subm_temp, cmt_temp, temp, authors\n",
    "\n",
    "    if axs.ndim == 1:\n",
    "        ax = axs[0]\n",
    "    elif axs.ndim == 2:\n",
    "        sub = np.unravel_index(cnt_query, (sub_idx[0], sub_idx[1]))\n",
    "        ax = axs[sub[0], sub[1]]\n",
    "        del sub   \n",
    "    \n",
    "    num_post = pd.DataFrame({'No. submissions': num_subm, 'No. comments': num_cmt, 'No. redditors':num_redditor}, index=name_months)\n",
    "    ax.plot(num_post['No. submissions'], color='#3d405b', label='No. submissions', linewidth=5)\n",
    "    ax.plot(num_post['No. comments'], color='#81b29a', label='No. comments', linewidth=5)\n",
    "    ax.plot(num_post['No. redditors'], color='#e07a5f', label='No. redditors', linewidth=5)\n",
    "    ax.axhline(y=num_files_month*100, color='r', linestyle='--', alpha=0.3)\n",
    "    ax.legend(loc='upper left', frameon=False, fontsize=15)\n",
    "    ax.set_xticklabels(name_months)\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('Number')\n",
    "    ax.set_title(query_main[cnt_query], fontsize=25)\n",
    "    ax.spines['right'].set_visible(0)\n",
    "    ax.spines['top'].set_visible(0)\n",
    "    del num_redditor, num_subm, num_cmt, num_post, ax\n",
    "    \n",
    "fig.set_size_inches(20*sub_idx[0], 6*sub_idx[1])\n",
    "if len(query_main) > 1:\n",
    "    plt.savefig(plt_cfg['path_save']+'stats_'+query_main[0]+'_etc.jpg', bbox_inches='tight')\n",
    "elif len(query_main) == 1:\n",
    "    plt.savefig(plt_cfg['path_save']+'stats_'+query_main[0]+'.jpg', bbox_inches='tight')\n",
    "del subm_cols, cmt_cols, num_files_month, idx_data, cnt_query, fig, axs, sub_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the text for word cloud\n",
    "Part of the code is adapted from the one originally produced by Zolzaya Luvsandorj ([Medium](https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)).  \n",
    "\n",
    "The following steps are performed in order:\n",
    "1. Concatenate all text from submission title, content and comments.\n",
    "2. Tokenize\n",
    "3. Normalize\n",
    "4. Remove stopwords\n",
    "5. Remove numbers, underscore, or words consist of less than two characters.\n",
    "6. Reverse processed words to a big paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, additional_stopwords=[]):\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # Lowercase and lemmatise \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if additional_stopwords != []:\n",
    "        keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')+additional_stopwords]\n",
    "    else:\n",
    "        keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "    \n",
    "    # Remove words with numbers, underscore, or words consist of less than three characters.\n",
    "    keywords = [word for word in keywords if not (any(char.isdigit() for char in word) or ('_' in word) or (len(word) < 3))]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = dict()\n",
    "for query in query_main:\n",
    "    word_freq[query] = []\n",
    "    for month in range(0, len(subms[query])):\n",
    "        # Concatenate text\n",
    "        subm = subms[query][month]\n",
    "        cmt = cmts[query][month]\n",
    "\n",
    "        txt = ''\n",
    "        for n in range(0, subm.shape[0]):\n",
    "            if type(subm['title'][n]) == str:\n",
    "                txt += subm['title'][n] + ' '\n",
    "            if type(subm['selftext'][n]) == str:      \n",
    "                txt += subm['selftext'][n] + ' '\n",
    "\n",
    "        for n in range(0, cmt.shape[0]):\n",
    "            if type(cmt['body'][n]) == str:\n",
    "                txt += cmt['body'][n]\n",
    "        \n",
    "        # Preprocess words\n",
    "        keywords = process_text(txt, add_stopwords)\n",
    "        \n",
    "        # Produce text frequency\n",
    "        word_freq[query].append({word: keywords.count(word) for word in set(keywords)})\n",
    "        del subm, cmt, keywords, txt\n",
    "    pickle.dump(word_freq[query], open(save_path_prefix+query+'/word_frequency.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordclouds = dict()\n",
    "for query in query_main:\n",
    "    fig, axs = plt.subplots(plt_cfg['xSub'], plt_cfg['ySub'])\n",
    "    wordclouds[query] = []\n",
    "    cnt = 0\n",
    "    for freq in word_freq[query]:\n",
    "        if len(freq) == 0:\n",
    "            wordclouds[query].append([])\n",
    "        else:\n",
    "            wordclouds[query].append(WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', \n",
    "                                               colormap='Set2').generate_from_frequencies(frequencies=freq))        \n",
    "            sub = np.unravel_index(cnt, (plt_cfg['xSub'], plt_cfg['ySub']))\n",
    "            ax = axs[sub[0], sub[1]]\n",
    "            ax.imshow(wordclouds[query][cnt])\n",
    "            ax.set_title(plt_cfg['title'][cnt], fontsize=20)\n",
    "            ax.spines['right'].set_visible(0)\n",
    "            ax.spines['top'].set_visible(0)\n",
    "            ax.axis('off')\n",
    "            del ax, sub\n",
    "        cnt += 1\n",
    "    fig.set_size_inches(plt_cfg['size'][0], plt_cfg['size'][1])\n",
    "    plt.savefig(plt_cfg['path_save']+query+'_1.jpg', bbox_inches='tight')\n",
    "    pickle.dump(wordclouds, open(save_path_prefix+query+'/wordclouds.p', 'wb'))\n",
    "del fig, axs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
