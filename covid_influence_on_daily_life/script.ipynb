{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_prefix = 'covid_influence/files/'\n",
    "query_keywords = ['covid|corona|quarantine|pandemic'] # Use '+' or '|' to connect multiple keywords; leave as [''] without searching for specific keywords\n",
    "query_subreddits = ['Gifts,GiftIdeas', 'personalfinance', 'jobs,careerguidence,GetEmployed', 'CasualConversation', 'depression', 'books', 'teenagers', 'parenting', 'fitness', \n",
    "                   'gaming', 'relationships'] # Use ',' to connect multiple subreddits, leave as [''] if search across all subreddits\n",
    "query_date_ranges = []\n",
    "for m in range(1, 13):\n",
    "    query_date_ranges.append(['2019-'+str(m).zfill(2)+'-01', '2019-'+str(m).zfill(2)+'-05'])\n",
    "    query_date_ranges.append(['2019-'+str(m).zfill(2)+'-06', '2019-'+str(m).zfill(2)+'-10'])\n",
    "    query_date_ranges.append(['2019-'+str(m).zfill(2)+'-11', '2019-'+str(m).zfill(2)+'-15'])\n",
    "    query_date_ranges.append(['2019-'+str(m).zfill(2)+'-16', '2019-'+str(m).zfill(2)+'-20'])\n",
    "    query_date_ranges.append(['2019-'+str(m).zfill(2)+'-21', '2019-'+str(m).zfill(2)+'-25'])\n",
    "    query_date_ranges.append(['2019-'+str(m).zfill(2)+'-26', '2019-'+str(m).zfill(2)+'-28'])\n",
    "\n",
    "# Parameters to plot the wordclouds\n",
    "name_months = ['Jan.', 'Feb.', 'Mar.', 'Apr.', 'May.', 'June', 'July', 'Aug.', 'Sept.', 'Oct.', 'Nov.', 'Dec.']\n",
    "plt_cfg = dict()\n",
    "plt_cfg['path_save'] = 'covid_influence/plots/'\n",
    "plt_cfg['size'] = (50, 25)\n",
    "plt_cfg['xSub'] = 3\n",
    "plt_cfg['ySub'] = 4\n",
    "plt_cfg['title'] = [month for month in name_months]\n",
    "\n",
    "# Parameters to fetch submissions and comments\n",
    "cfg_subm = dict()\n",
    "cfg_subm['field'] = 'title,selftext,num_comments,author'\n",
    "cfg_subm['rm_dupe'] = 'title'\n",
    "cfg_subm['sort'] = 'num_comments'\n",
    "cfg_subm['sort_type'] = 'desc'\n",
    "cfg_subm['query_type'] = 'submission'\n",
    "\n",
    "cfg_cmt = dict()\n",
    "cfg_cmt['field'] = 'body,author,score'\n",
    "cfg_cmt['rm_dupe'] = 'body'\n",
    "cfg_cmt['sort'] = 'score'\n",
    "cfg_cmt['sort_type'] = 'desc'\n",
    "cfg_cmt['query_type'] = 'comment'\n",
    "\n",
    "# Add additional stopwords\n",
    "add_stopwords = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'day', 'week', 'month', 'year', 'thing', 'app', 'new', 'old', \n",
    "                 'hundred', 'thousand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcs_pushshift\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting submissions and comments from reddit using [Pushshift](https://reddit-api.readthedocs.io/en/latest/#comments-search)\n",
    "\n",
    "The original code of the function *GetPushshiftData* is from [\n",
    "dylankilkenny/PushShift.py](https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b) and [Rare Loot](https://rareloot.medium.com/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563).\n",
    "\n",
    "### 1. Assemble queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = []\n",
    "fname = []\n",
    "query_main = []\n",
    "query_subm = []\n",
    "query_cmt = []\n",
    "suffix_subm = '&filter='+cfg_subm['field']+'&sort_type='+cfg_subm['sort']+'&sort=desc'+'&size=100'\n",
    "suffix_cmt = '&filter='+cfg_cmt['field']+'&sort_type='+cfg_cmt['sort']+'&sort=desc'+'&size=100'\n",
    "for keyword in query_keywords:\n",
    "    query_temp = 'q='+keyword\n",
    "    for subreddit in query_subreddits:\n",
    "        query_main.append(keyword+'_'+subreddit)\n",
    "        for date_range in query_date_ranges:\n",
    "            query_subm.append(query_temp+'&subreddit='+subreddit+'&after='+ date_range[0]+'&before='+date_range[1]+suffix_subm)\n",
    "            query_cmt.append(query_temp+'&subreddit='+subreddit+'&after='+ date_range[0]+'&before='+date_range[1]+suffix_cmt)\n",
    "            save_path.append(save_path_prefix+keyword+'_'+subreddit+'/')\n",
    "            fname.append(date_range[0]+'_'+date_range[1])\n",
    "    del query_temp\n",
    "del suffix_subm, suffix_cmt, keyword, subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code\n",
    "# subms_all = []\n",
    "# cmts_all = []\n",
    "# for idx in range(len(fname)):\n",
    "#     try:\n",
    "#         subms_all.append(pd.read_csv(save_path[idx]+cfg_subm['query_type']+'_'+fname[idx]+'.csv'))\n",
    "#     except:\n",
    "#         subms_all.append([])\n",
    "#     try:\n",
    "#         cmts_all.append(pd.read_csv(save_path[idx]+cfg_cmt['query_type']+'_'+fname[idx]+'.csv'))\n",
    "#     except:\n",
    "#         cmts_all.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collect the submissions and comments from Pushshift server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subms_all = []\n",
    "cmts_all = []\n",
    "for idx in range(0, len(fname)):\n",
    "    print('> Processing query: '+str(idx+1)+' / '+str(len(query_subm)) + '. Save to: '+save_path[idx]+fname[idx]+'/')\n",
    "    cfg_subm['path_save'] = save_path[idx]\n",
    "    cfg_subm['save_suffix'] = fname[idx]\n",
    "    df_subm = funcs_pushshift.fetch_data(query_subm[idx], cfg_subm)\n",
    "    subms_all.append(df_subm)\n",
    "\n",
    "    cfg_cmt['path_save'] = save_path[idx]\n",
    "    cfg_cmt['save_suffix'] = fname[idx]\n",
    "    df_cmt = funcs_pushshift.fetch_data(query_cmt[idx], cfg_cmt)\n",
    "    cmts_all.append(df_cmt)\n",
    "    del df_subm, df_cmt\n",
    "del query_subm, query_cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assemble text of each month and calculate the number of active redditors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_dividor(n):\n",
    "    tmp = []\n",
    "    i = 1\n",
    "    while i <= n: \n",
    "        if (n % i==0) : \n",
    "            tmp.append(i) \n",
    "        i = i + 1\n",
    "    if len(tmp) % 2 == 0:\n",
    "        return [tmp[int(len(tmp)/2-1)], tmp[int(len(tmp)/2)]]\n",
    "    else:\n",
    "        return [tmp[round(len(tmp)/2)], tmp[round(len(tmp)/2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for plotting\n",
    "if not os.path.exists(plt_cfg['path_save']):\n",
    "    os.makedirs(plt_cfg['path_save'])\n",
    "sub_idx = mid_dividor(len(query_main))\n",
    "fig, axs = plt.subplots(sub_idx[0], sub_idx[1])\n",
    "\n",
    "# Get column names\n",
    "subm_cols = cfg_subm['field'].split(',')\n",
    "cmt_cols = cfg_cmt['field'].split(',')\n",
    "# Combine the data from the same month for each year and calculate the number of active redditors\n",
    "num_files_month = int(len(fname) / (len(name_months) * len(query_keywords) * len(query_subreddits)))\n",
    "subms = dict()\n",
    "cmts = dict()\n",
    "idx_data = 0\n",
    "for cnt_query in range(len(query_main)):\n",
    "    subms[query_main[cnt_query]] = []\n",
    "    cmts[query_main[cnt_query]] = []\n",
    "    num_redditor = []\n",
    "    num_subm = []\n",
    "    num_cmt = []\n",
    "    for idx in range(0, len(name_months)):\n",
    "        subm_temp = []\n",
    "        cmt_temp = []\n",
    "        for ii in range(0, num_files_month):\n",
    "            if subms_all[idx_data].shape[0] != 0:\n",
    "                subm_temp.append(subms_all[idx_data])\n",
    "            if cmts_all[idx_data].shape[0] != 0:\n",
    "                cmt_temp.append(cmts_all[idx_data])\n",
    "            idx_data += 1\n",
    "        if subm_temp == []:\n",
    "            subms[query_main[cnt_query]].append(pd.DataFrame(columns=subm_cols))\n",
    "        else:\n",
    "            subms[query_main[cnt_query]].append(pd.concat(subm_temp).reset_index()[subm_cols])\n",
    "        if cmt_temp == []:\n",
    "            cmts[query_main[cnt_query]].append(pd.DataFrame(columns=subm_cols))\n",
    "        else:\n",
    "            cmts[query_main[cnt_query]].append(pd.concat(cmt_temp).reset_index()[cmt_cols])\n",
    "\n",
    "        # Calculate the number of redditors\n",
    "        authors = pd.concat([subms[query_main[cnt_query]][idx]['author'], cmts[query_main[cnt_query]][idx]['author']])\n",
    "        temp = authors.shape[0]\n",
    "        authors = authors.drop_duplicates().reset_index()\n",
    "        num_redditor.append(authors.shape[0])\n",
    "        if len(subms[query_main[cnt_query]][idx]) == 0:\n",
    "            num_subm.append(0)\n",
    "        else:\n",
    "            num_subm.append(len(subms[query_main[cnt_query]][idx]))\n",
    "        num_cmt.append(len(cmts[query_main[cnt_query]][idx]))\n",
    "        del subm_temp, cmt_temp, temp, authors\n",
    "\n",
    "    if axs.ndim == 1:\n",
    "        ax = axs[0]\n",
    "    elif axs.ndim == 2:\n",
    "        sub = np.unravel_index(cnt_query, (sub_idx[0], sub_idx[1]))\n",
    "        ax = axs[sub[0], sub[1]]\n",
    "        del sub   \n",
    "    \n",
    "    num_post = pd.DataFrame({'No. submissions': num_subm, 'No. comments': num_cmt, 'No. redditors':num_redditor}, index=name_months)\n",
    "    ax.plot(num_post['No. submissions'], color='#3d405b', label='No. submissions', linewidth=5)\n",
    "    ax.plot(num_post['No. comments'], color='#81b29a', label='No. comments', linewidth=5)\n",
    "    ax.plot(num_post['No. redditors'], color='#e07a5f', label='No. redditors', linewidth=5)\n",
    "    ax.axhline(y=num_files_month*100, color='r', linestyle='--', alpha=0.3)\n",
    "    ax.legend(loc='upper left', frameon=False, fontsize=15)\n",
    "    ax.set_xticklabels(name_months)\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('Number')\n",
    "    ax.set_title(query_main[cnt_query], fontsize=25)\n",
    "    ax.spines['right'].set_visible(0)\n",
    "    ax.spines['top'].set_visible(0)\n",
    "    del num_redditor, num_subm, num_cmt, num_post, ax\n",
    "    \n",
    "fig.set_size_inches(20*sub_idx[0], 6*sub_idx[1])\n",
    "if len(query_main) > 1:\n",
    "    plt.savefig(plt_cfg['path_save']+'stats_'+query_main[0]+'_etc.jpg', bbox_inches='tight')\n",
    "elif len(query_main) == 1:\n",
    "    plt.savefig(plt_cfg['path_save']+'stats_'+query_main[0]+'.jpg', bbox_inches='tight')\n",
    "del subm_cols, cmt_cols, num_files_month, idx_data, cnt_query, fig, axs, sub_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the text for word cloud\n",
    "Part of the code is adapted from the one originally produced by Zolzaya Luvsandorj ([Medium](https://towardsdatascience.com/introduction-to-nlp-part-1-preprocessing-text-in-python-8f007d44ca96)).  \n",
    "\n",
    "The following steps are performed in order:\n",
    "1. Concatenate all text from submission title, content and comments.\n",
    "2. Tokenize\n",
    "3. Normalize\n",
    "4. Remove stopwords\n",
    "5. Remove numbers, underscore, or words consist of less than two characters.\n",
    "6. Reverse processed words to a big paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, additional_stopwords=[]):\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # Lowercase and lemmatise \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if additional_stopwords != []:\n",
    "        keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')+additional_stopwords]\n",
    "    else:\n",
    "        keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "    \n",
    "    # Remove words with numbers, underscore, or words consist of less than three characters.\n",
    "    keywords = [word for word in keywords if not (any(char.isdigit() for char in word) or ('_' in word) or (len(word) < 3))]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = dict()\n",
    "for query in query_main:\n",
    "    word_freq[query] = []\n",
    "    for month in range(0, len(subms[query])):\n",
    "        # Concatenate text\n",
    "        subm = subms[query][month]\n",
    "        cmt = cmts[query][month]\n",
    "\n",
    "        txt = ''\n",
    "        for n in range(0, subm.shape[0]):\n",
    "            if type(subm['title'][n]) == str:\n",
    "                txt += subm['title'][n] + ' '\n",
    "            if type(subm['selftext'][n]) == str:      \n",
    "                txt += subm['selftext'][n] + ' '\n",
    "\n",
    "        for n in range(0, cmt.shape[0]):\n",
    "            if type(cmt['body'][n]) == str:\n",
    "                txt += cmt['body'][n]\n",
    "        \n",
    "        # Preprocess words\n",
    "        keywords = process_text(txt, add_stopwords)\n",
    "        \n",
    "        # Produce text frequency\n",
    "        word_freq[query].append({word: keywords.count(word) for word in set(keywords)})\n",
    "        del subm, cmt, keywords, txt\n",
    "    pickle.dump(word_freq[query], open(save_path_prefix+query+'/word_frequency.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordclouds = dict()\n",
    "for query in query_main:\n",
    "    fig, axs = plt.subplots(plt_cfg['xSub'], plt_cfg['ySub'])\n",
    "    wordclouds[query] = []\n",
    "    cnt = 0\n",
    "    for freq in word_freq[query]:\n",
    "        if len(freq) == 0:\n",
    "            wordclouds[query].append([])\n",
    "        else:\n",
    "            wordclouds[query].append(WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', \n",
    "                                               colormap='Set2').generate_from_frequencies(frequencies=freq))        \n",
    "            sub = np.unravel_index(cnt, (plt_cfg['xSub'], plt_cfg['ySub']))\n",
    "            ax = axs[sub[0], sub[1]]\n",
    "            ax.imshow(wordclouds[query][cnt])\n",
    "            ax.set_title(plt_cfg['title'][cnt], fontsize=20)\n",
    "            ax.spines['right'].set_visible(0)\n",
    "            ax.spines['top'].set_visible(0)\n",
    "            ax.axis('off')\n",
    "            del ax, sub\n",
    "        cnt += 1\n",
    "    fig.set_size_inches(plt_cfg['size'][0], plt_cfg['size'][1])\n",
    "    plt.savefig(plt_cfg['path_save']+query+'.jpg', bbox_inches='tight')\n",
    "    pickle.dump(wordclouds, open(save_path_prefix+query+'/wordclouds.p', 'wb'))\n",
    "del fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
