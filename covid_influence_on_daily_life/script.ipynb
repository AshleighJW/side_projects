{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How COVID Influenced People's lives\n",
    "\n",
    "COVID has a great negative influence on the society. Nearly nine-in-ten U.S. adults say their life has changed at least a little as a result of the COVID-19 outbreak, including 44% who say their life has changed in a major way ([Most Americans Say Coronavirus Outbreak Has Impacted Their Lives](https://www.pewsocialtrends.org/2020/03/30/most-americans-say-coronavirus-outbreak-has-impacted-their-lives/)). This study quantifies how COVID has influenced many aspects of people's everyday life by visualizing keywords changes in subreddits concerning daily life topics during the past year (in case you are not familiar with reddit: [subreddit is a web forum of a particular topic where you can post links or create a self post and dicuss.](https://www.reddit.com/r/help/comments/37shum/what_is_a_subreddit/)).\n",
    "\n",
    "### Structure\n",
    "What's included in the analysis: trace the number of members of each subreddit and the keywords change.\n",
    "\n",
    "### Data source\n",
    "The data is scrapped from one of the biggest online community **reddit**. **Reddit** has more than [430m users](https://au.oberlo.com/blog/reddit-statistics), [168b views per year](https://mediakix.com/blog/reddit-statistics-users-demographics/)).\n",
    "\n",
    "[Aother good statistics](https://websitebuilder.org/blog/reddit-statistics/).\n",
    "\n",
    "### Subreddits selected\n",
    "**Selection criteria**\n",
    "- Focus on a daily life related topics, as listed separatly below.\n",
    "- Among all subreddits with similar topic, I select the one or two with most members to idealize the size and variety of the data.\n",
    "\n",
    "**List of subreddits (click the name to view the subreddit)**\n",
    "\n",
    "- **Food**   \n",
    "[r/cooking](https://www.reddit.com/r/Cooking/): 2.5m members, [r/recipes](https://www.reddit.com/r/recipes/): 2.4m members, [r/MealPrepSunday](https://www.reddit.com/r/MealPrepSunday/): 2m members, [r/budgetfood](https://www.reddit.com/r/budgetfood/): 302k members\n",
    "\n",
    "- **Family life, childcare and relationships**  \n",
    "[r/Parenting](https://www.reddit.com/r/Parenting/): 3.2m members, [r/relationships](https://www.reddit.com/r/relationships/): 3m members, [r/teenagers](https://www.reddit.com/r/teenagers/): 2.4m members, [r/childfree](https://www.reddit.com/r/childfree/): 1.4m members, [r/weddingplanning](https://www.reddit.com/r/weddingplanning/): 144k members, [r/family](https://www.reddit.com/r/family/): 138k members, [r/education](https://www.reddit.com/r/education/new/): 124k members, [r/christmas](https://www.reddit.com/r/christmas/): 100k members, [r/Gifts](https://www.reddit.com/r/Gifts/): 56.2k members, [r/GiftIdeas](https://www.reddit.com/r/GiftIdeas/): 45.4k members\n",
    "\n",
    "- **Hobbies**  \n",
    "[r/gaming](https://www.reddit.com/r/gaming/): 29.3m members, [r/books](https://www.reddit.com/r/books/): 19m members, [r/camping](https://www.reddit.com/r/camping/): 1.9m members, [r/Fitness](https://www.reddit.com/r/Fitness/): 8m members, [r/travel](https://www.reddit.com/r/travel/): 5.7m members\n",
    "\n",
    "- **Financial status**\n",
    "[r/personalfinance](https://www.reddit.com/r/personalfinance/): 14.3m members, [r/Economics](https://www.reddit.com/r/Economics/): 1.2m members, [r/povertyfinance](https://www.reddit.com/r/povertyfinance/): 588k members, [r/CreditCards](https://www.reddit.com/r/CreditCards/): 262k members, [r/realestateinvesting](https://www.reddit.com/r/realestateinvesting/): 330k members, [r/RealEstate](https://www.reddit.com/r/RealEstate/): 236k members\n",
    "\n",
    "- **Career**\n",
    "[r/Entrepreneur](https://www.reddit.com/r/Entrepreneur/): 889k members, [r/business](https://www.reddit.com/r/business/): 602k members, [r/smallbusiness](https://www.reddit.com/r/smallbusiness/): 484k members, [r/jobs](https://www.reddit.com/r/jobs/): 428k members, [r/WorkOnline](https://www.reddit.com/r/WorkOnline/): 333k members, [r/careerguidance](https://www.reddit.com/r/careerguidance/): 250k members, [r/GetEmployed](https://www.reddit.com/r/GetEmployed/): 102k members\n",
    "\n",
    "- **Mental Health**\n",
    "[r/GetMotivated](https://www.reddit.com/r/GetMotivated/): 17.0m members, [r/CasualConversation](https://www.reddit.com/r/CasualConversation/): 1.4m members, [r/psychology](https://www.reddit.com/r/psychology/): 809k members, [r/getdisciplined](https://www.reddit.com/r/getdisciplined/): 729k members, [r/depression](https://www.reddit.com/r/depression/): 725k members, [r/Anxiety](https://www.reddit.com/r/Anxiety/): 429k members, [r/mentalhealth](https://www.reddit.com/r/mentalhealth/): 220k members, [r/Lonely](https://www.reddit.com/r/lonely/): 194k members\n",
    "\n",
    "- **Others**\n",
    "[r/Futurology](https://www.reddit.com/r/Futurology/): 15.1m members, [r/Coronavirus](https://www.reddit.com/r/Coronavirus/): 2.4m members, [r/legaladvice](https://www.reddit.com/r/legaladvice/): 1.4m members\n",
    "\n",
    "### Major tool\n",
    "- **Pushshift** ([Manual](https://reddit-api.readthedocs.io/en/latest/), [Github](https://github.com/pushshift/api), [r/Pushshift](https://www.reddit.com/r/pushshift/))  \n",
    "\n",
    "Pushshift is used to scrapping data from reddit. Compared to the popular tool [PRAW](https://praw.readthedocs.io/en/latest/), Pushshift provides a more powerful way to search the submissions and comments, especially for the date related search queries. In addtion, Pushshift returns query results quicker when the search invloves large amount of data.\n",
    "\n",
    "On the other hand, the research results from Pushshift have a couple day delay - meaning that the lastest reddit data you can get using Pushshift is about a couple days ago. This analysis, though, is not real-time, therefore I used Pushshift to get the data I need for this analysis.\n",
    "\n",
    "- **[WordCloud](https://amueller.github.io/word_cloud/index.html)**  \n",
    "\n",
    "WordCloud is used to visualize the keywords of a specific (group of) subreddit(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.reddit.com/r/plants/about\n",
    "#### Country Selection\n",
    "1. Reddit ranks 0-30 websites in the country\n",
    "    https://www.reddit.com/r/MapPorn/comments/61vzin/popularity_of_reddit_by_country_4500_x_2234/\n",
    "    https://www.statista.com/statistics/325144/reddit-global-active-user-distribution/\n",
    "2. English speaking? (Also be mindful of population size)\n",
    "3. Has enough posts to produce a valid word cloud\n",
    "4. Grouped by COVID situation.\n",
    "\n",
    "- **[r/PS4](https://www.reddit.com/r/PS4/)**: 4.1m members\n",
    "- **[r/xboxone](https://www.reddit.com/r/xboxone/)**: 2.7m members\n",
    "- **[r/pcgaming](https://www.reddit.com/r/pcgaming/)**: 2.5m members\n",
    "- **[r/nintendo](https://www.reddit.com/r/nintendo/)**: 2m members\n",
    "- **[r/Games](https://www.reddit.com/r/Games/)**: 2.8m members\n",
    "- **[r/battlestations](https://www.reddit.com/r/battlestations/)**: 2.3m members\n",
    "- **[r/Shoestring](https://www.reddit.com/r/Shoestring/)**: 1.1m members\n",
    "- **[r/solotravel](https://www.reddit.com/r/solotravel/)**: 1.5m members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_prefix = 'covid_influence/files/'\n",
    "query_keywords = ['covid|corona|quarantine|pandemic'] # Use '+' or '|' to connect multiple keywords; leave as [''] without searching for specific keywords\n",
    "query_subreddits = ['GetMotivated'] # Use ',' to connect multiple subreddits, leave as [''] if search across all subreddits\n",
    "query_date_ranges = []\n",
    "for m in range(1, 13):\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-01', '2020-'+str(m).zfill(2)+'-05'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-06', '2020-'+str(m).zfill(2)+'-10'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-11', '2020-'+str(m).zfill(2)+'-15'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-16', '2020-'+str(m).zfill(2)+'-20'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-21', '2020-'+str(m).zfill(2)+'-25'])\n",
    "    query_date_ranges.append(['2020-'+str(m).zfill(2)+'-26', '2020-'+str(m).zfill(2)+'-28'])\n",
    "\n",
    "# Parameters to plot the wordclouds\n",
    "name_months = ['Jan.', 'Feb.', 'Mar.', 'Apr.', 'May.', 'June', 'July', 'Aug.', 'Sept.', 'Oct.', 'Nov.', 'Dec.']\n",
    "plt_cfg = dict()\n",
    "plt_cfg['path_save'] = 'covid_influence/plots/'\n",
    "plt_cfg['size'] = (50, 25)\n",
    "plt_cfg['xSub'] = 3\n",
    "plt_cfg['ySub'] = 4\n",
    "# plt_cfg['idx_splot'] = list(range(1, len(name_months)+1))\n",
    "plt_cfg['title'] = [month for month in name_months]\n",
    "\n",
    "# Parameters to fetch submissions and comments\n",
    "cfg_subm = dict()\n",
    "cfg_subm['field'] = 'title,selftext,num_comments,author'\n",
    "cfg_subm['rm_dupe'] = 'title'\n",
    "cfg_subm['sort'] = 'num_comments'\n",
    "cfg_subm['sort_type'] = 'desc'\n",
    "cfg_subm['query_type'] = 'submission'\n",
    "\n",
    "cfg_cmt = dict()\n",
    "cfg_cmt['field'] = 'body,author,score'\n",
    "cfg_cmt['rm_dupe'] = 'body'\n",
    "cfg_cmt['sort'] = 'score'\n",
    "cfg_cmt['sort_type'] = 'desc'\n",
    "cfg_cmt['query_type'] = 'comment'\n",
    "\n",
    "# Add additional stopwords\n",
    "add_stopwords = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'day', 'week', 'month', 'year', 'thing', 'app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcs_pushshift\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting submissions and comments from reddit using [Pushshift](https://reddit-api.readthedocs.io/en/latest/#comments-search)\n",
    "\n",
    "The original code of the function *GetPushshiftData* is from [\n",
    "dylankilkenny/PushShift.py](https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b) and [Rare Loot](https://rareloot.medium.com/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563).\n",
    "\n",
    "### 1. Assemble queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = []\n",
    "fname = []\n",
    "query_main = []\n",
    "query_subm = []\n",
    "query_cmt = []\n",
    "suffix_subm = '&filter='+cfg_subm['field']+'&sort_type='+cfg_subm['sort']+'&sort=desc'+'&size=100'\n",
    "suffix_cmt = '&filter='+cfg_cmt['field']+'&sort_type='+cfg_cmt['sort']+'&sort=desc'+'&size=100'\n",
    "for keyword in query_keywords:\n",
    "    query_temp = 'q='+keyword\n",
    "    for subreddit in query_subreddits:\n",
    "        query_main.append(keyword+'_'+subreddit)\n",
    "        query_temp += '&subreddit='+subreddit\n",
    "        for date_range in query_date_ranges:\n",
    "            query_subm.append(query_temp+'&after='+ date_range[0]+'&before='+date_range[1]+suffix_subm)\n",
    "            query_cmt.append(query_temp+'&after='+ date_range[0]+'&before='+date_range[1]+suffix_cmt)\n",
    "            save_path.append(save_path_prefix+keyword+'_'+subreddit+'/')\n",
    "            fname.append(date_range[0]+'_'+date_range[1])\n",
    "    del query_temp\n",
    "del suffix_subm, suffix_cmt, keyword, subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collect the submissions and comments from Pushshift server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subms_all = []\n",
    "cmts_all = []\n",
    "for idx in range(0, len(fname)):\n",
    "    print('> Processing query: '+str(idx+1)+' / '+str(len(query_subm)) + '. Save to: '+save_path[idx]+fname[idx]+'/')\n",
    "    cfg_subm['path_save'] = save_path[idx]\n",
    "    cfg_subm['save_suffix'] = fname[idx]\n",
    "    df_subm = funcs_pushshift.fetch_data(query_subm[idx], cfg_subm)\n",
    "    subms_all.append(df_subm)\n",
    "\n",
    "    cfg_cmt['path_save'] = save_path[idx]\n",
    "    cfg_cmt['save_suffix'] = fname[idx]\n",
    "    df_cmt = funcs_pushshift.fetch_data(query_cmt[idx], cfg_cmt)\n",
    "    cmts_all.append(df_cmt)\n",
    "    del df_subm, df_cmt\n",
    "del query_subm, query_cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assemble text of each month and calculate the number of active redditors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for plotting\n",
    "if not os.path.exists(plt_cfg['path_save']):\n",
    "    os.makedirs(plt_cfg['path_save'])\n",
    "fig, axs = plt.subplots(len(query_keywords), len(query_subreddits))\n",
    "\n",
    "# Get column names\n",
    "subm_cols = cfg_subm['field'].split(',')\n",
    "cmt_cols = cfg_cmt['field'].split(',')\n",
    "# Combine the data from the same month for each year and calculate the number of active redditors\n",
    "num_files_year = int(len(fname) / (len(name_months) * len(query_keywords) * len(query_subreddits)))\n",
    "subms = dict()\n",
    "cmts = dict()\n",
    "num_redditor = []\n",
    "idx_data = 0\n",
    "for cnt_query in range(len(query_main)):\n",
    "    subms[query_main[cnt_query]] = []\n",
    "    cmts[query_main[cnt_query]] = []\n",
    "    for idx in range(0, len(name_months)):\n",
    "        subm_temp = []\n",
    "        cmt_temp = []\n",
    "        for ii in range(0, num_files_year):\n",
    "            if len(subms_all[idx_data]) != 0:\n",
    "                subm_temp.append(subms_all[idx_data])\n",
    "            if len(cmts_all[idx_data]) != 0:\n",
    "                cmt_temp.append(cmts_all[idx_data])\n",
    "            idx_data += 1\n",
    "        subms[query_main[cnt_query]].append(pd.concat(subm_temp).reset_index()[subm_cols])\n",
    "        cmts[query_main[cnt_query]].append(pd.concat(cmt_temp).reset_index()[cmt_cols])\n",
    "\n",
    "        # Calculate the number of redditors\n",
    "        authors = pd.concat([subms[query_main[cnt_query]][idx]['author'], cmts[query_main[cnt_query]][idx]['author']])\n",
    "        temp = authors.shape[0]\n",
    "        authors = authors.drop_duplicates().reset_index()\n",
    "        num_redditor.append(authors.shape[0])\n",
    "        del temp, authors\n",
    "\n",
    "    num_redditor = pd.DataFrame({'data':num_redditor}, index=name_months)\n",
    "    if len(query_keywords)*len(query_subreddits) == 1:\n",
    "        ax = axs\n",
    "    else:\n",
    "        ax = axs[cnt_query]\n",
    "    ax.plot(num_redditor['data'], color='#006600')\n",
    "    ax.set_xticklabels(name_months)\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('No. of Activer Redditor')\n",
    "    ax.set_title(query_main[cnt_query], fontsize=15)\n",
    "    ax.spines['right'].set_visible(0)\n",
    "    ax.spines['top'].set_visible(0)\n",
    "    del subm_temp, cmt_temp, num_redditor, ax\n",
    "    \n",
    "# plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "fig.set_size_inches(10, 6)\n",
    "plt.savefig(plt_cfg['path_save']+'num_redditor_change'+query_main[0]+'.jpg', bbox_inches='tight')\n",
    "del subm_cols, cmt_cols, num_files_year, idx_data, cnt_query, fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the text for word cloud\n",
    "Part of the code is adapted from the one originally produced by Zolzaya Luvsandorj ([Medium](https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5)).  \n",
    "\n",
    "The following steps are performed in order:\n",
    "1. Concatenate all text from submission title, content and comments.\n",
    "2. Tokenize\n",
    "3. Normalize\n",
    "4. Remove stopwords\n",
    "5. Remove numbers, underscore, or words consist of less than two characters.\n",
    "6. Reverse processed words to a big paragraph of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text = dict()\n",
    "for query in query_main:\n",
    "    text[query] = []\n",
    "    for month in range(0, len(subms[query])):\n",
    "        # 1. Concatenate text\n",
    "        subm = subms[query][month]\n",
    "        cmt = cmts[query][month]\n",
    "\n",
    "        txt = ''\n",
    "        for n in range(0, subm.shape[0]):\n",
    "            if type(subm['title'][n]) == str:\n",
    "                txt += subm['title'][n] + ' '\n",
    "            if type(subm['selftext'][n]) == str:      \n",
    "                txt += subm['selftext'][n] + ' '\n",
    "\n",
    "        for n in range(0, cmt.shape[0]):\n",
    "            if type(cmt['body'][n]) == str:\n",
    "                txt += cmt['body'][n]\n",
    "\n",
    "        # 2. Tokenize\n",
    "        tokeniser = RegexpTokenizer(r'\\w+') # Create an instance of RegexpTokenizer for alphanumeric tokens\n",
    "        tokens = tokeniser.tokenize(txt) # Tokenise 'part1' string\n",
    "\n",
    "        # 3. Normalize\n",
    "        lemmatiser = WordNetLemmatizer() # Create an instance of WordNetLemmatizer\n",
    "        lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "        lemmas = list(set(lemmas))\n",
    "\n",
    "        # 4. Remove stop words\n",
    "        keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "        keywords = [lemma for lemma in lemmas if lemma not in add_stopwords]\n",
    "\n",
    "        # 5. Remove numbers, underscore, or words consist of less than two characters.\n",
    "        keywords = [word for word in keywords if not (any(char.isdigit() for char in word) or ('_' in word) or (len(word) < 2))]\n",
    "\n",
    "        # 6. Reverse processed words to a big paragraph of text.\n",
    "        txt = ''\n",
    "        for m in range(0, len(keywords)):\n",
    "            txt += keywords[m] + ' '\n",
    "        text[query].append(txt)\n",
    "        del subm, cmt, tokeniser, tokens, lemmatiser, lemmas, keywords, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of words included in each month.\n",
    "print('- Number of words included in each month ('+query_main[0]+')')\n",
    "num_word_temp = []\n",
    "for txt in text[query_main[0]]:\n",
    "    num_word_temp.append(len(txt))\n",
    "\n",
    "num_word = pd.DataFrame({'2019': num_word_temp}, index=name_months)\n",
    "num_word.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordclouds = dict()\n",
    "\n",
    "for query in query_main:\n",
    "    fig, axs = plt.subplots(plt_cfg['xSub'], plt_cfg['ySub'])\n",
    "    wordclouds[query] = []\n",
    "    cnt = 0\n",
    "    for txt in text[query]:\n",
    "        wordclouds[query].append(WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(txt))\n",
    "        \n",
    "        sub = np.unravel_index(cnt, (plt_cfg['xSub'], plt_cfg['ySub']))\n",
    "        ax = axs[sub[0], sub[1]]\n",
    "        ax.imshow(wordclouds[query][cnt])\n",
    "        ax.set_title(plt_cfg['title'][cnt], fontsize=20)\n",
    "        ax.spines['right'].set_visible(0)\n",
    "        ax.spines['top'].set_visible(0)\n",
    "        ax.axis('off')\n",
    "        cnt += 1\n",
    "        del ax, sub\n",
    "    \n",
    "#     plt.rcParams[\"figure.figsize\"] = plt_cfg['size']\n",
    "    fig.set_size_inches(plt_cfg['size'][0], plt_cfg['size'][1])\n",
    "    plt.savefig(plt_cfg['path_save']+query+'.jpg', bbox_inches='tight')\n",
    "    pickle.dump(wordclouds, open(save_path_prefix+query+'/wordclouds.p', 'wb'))\n",
    "del fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
